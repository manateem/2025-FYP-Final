[Decision] Combine our previous mandatory assignment groups' manually-annotated datasets into one large dataset, and drop the group_id column -> 400 images lots of data; the values for each are already group consensus between 5 people, no need to add 2 more people for each for the members who were not in that particular group
[Decision] Put the logbook full of decisions on the Github because decisions are usually made as a group and logs are usually fixed and static with only additions needed; this is compared to Overleaf which may be more confusing; to put things all in one place.
[Decision] Every meeting we congregate to discuss what we've done, review each other's work, and decide where to go from here. Then we add all our new tasks to the todo.md file. Then from here we can split up work and do the tasks individually, then repeat, meeting regularly.
[Decision] The open question is regarding The Curse of Multidimensionality: How many features can we cram into one classifier, and how does that affect results? What if we remove the less important factors and then train again? What if we only test groups of features (image features, metadata features, etc.)? Then test our baseline, extended, mega-classifier, and adjusted mega-classifiers and analyze our results.
[Decision] We decided that we want to first start with a Baseline ABC classifier, training for the purpose of creating logistic regression, KNN, and decision tree classifiers. Afterwards, we wish to utilize two models to explore the curse of dimensionality. In one model, we are maximizing for the sake of the number of features. This will likely reduce our sample size to 1000, but will open us up to a large number of features from the metadata, once again training for the purpose of the 3 classifiers. In the other model, we are maximizing for the sake of the sample size, while still maintaining a large number of features by focusing mainly on image features and using an external dataset JUST FOR TESTING the classifiers to account for overfitting (which the curse of dimensionality is said to induce), and training again for the purpose of 3 classifiers. Finally, we create a (one/two) final models, reducing the amount of redundant features we use in the previous "mega-classifiers". As a control case, we will also add a variety of random features to a model and train it. In the end, we finally look at the test data and test our classifiers, and compare their performances.